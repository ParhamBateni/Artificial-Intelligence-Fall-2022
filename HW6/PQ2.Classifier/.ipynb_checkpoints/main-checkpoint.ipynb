{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "In God We Trust\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE417: Artificial Intelligence\n",
    "\n",
    "Dr. Mahdiyeh Soleymani Baghshah, Associate Professor\n",
    "\n",
    "Computer Engineering Department,\n",
    "Sharif University of Technology,\n",
    "Tehran, Tehran, Iran\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier (20 Points)\n",
    "\n",
    "Corresponding TA: Pourya Momtaz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkmsZYLH5ePw"
   },
   "source": [
    "During this assignment, the collective results of all your HITs as a class will be used as datasets to train image classifier. We will be using [fastai](https://docs.fast.ai/), which provides straightforward methods for deep learning. Deep learning utilizes multiple layers of neural networks in order to extract and transform data. Applications of deep learning can be found all around you, including speech recognition, autonomous driving, and board games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4kZ2E7rF5cY"
   },
   "source": [
    "# Install dependencies\n",
    "\n",
    "You only need to run this once to set the notebook up. Make sure you select Runtime > Change Runtime Type > GPU to get a GPU on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jDtZ6UXuF-az"
   },
   "outputs": [],
   "source": [
    "#hide\n",
    "!pip install -Uqq fastbook\n",
    "import fastbook\n",
    "fastbook.setup_book()\n",
    "import sklearn\n",
    "import os\n",
    "\n",
    "#hide\n",
    "from fastbook import *\n",
    "\n",
    "#hide\n",
    "! curl https://nets213-hw5.s3.amazonaws.com/mega_results_cleaned_validated.csv -o results.csv \n",
    "! curl https://nets213-hw5.s3.amazonaws.com/weddings-indian-languages.zip -o  weddings-indian-languages.zip\n",
    "! curl https://nets213-hw5.s3.amazonaws.com/weddings%2Brandom-european-language.zip -o weddings-european-language.zip\n",
    "! unzip -o weddings-indian-languages.zip 1>/dev/null\n",
    "! unzip -o weddings-european-language.zip 1>/dev/null\n",
    "! rm -rf weddings-european-language 1>/dev/null 2>/dev/null || true\n",
    "os.rename(\"weddings+random-european-language\",\"weddings-european-language\") # renames the folder for convenience later in the assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abruOTklINRu"
   },
   "source": [
    "# Load all the compiled HITs results\n",
    "\n",
    "We've provided **mega_results_cleaned_validated.csv**, which is a compilation of all student HIT results. Read it in and for each image, calculate the count of how many Turkers voted yes or no for the image. Note that in order to improve the classifiers' performances, several hundred negative Western images were added to augment the Western dataset of images. These additional Western images were all labeled to not depict weddings.\n",
    "\n",
    "As you were assigned to randomly shuffle your images, the actual number of votes that each image received isn't uniform. Thus, in the following cells, we will classify an image as True (the image depicts a wedding) if the majority of its votes are yes. For example, if an image received a total of seven votes, if at least four workers voted yes, then we will classify the image as True. If an image received a total of eight votes, if at least five workers voted yes, then we will classify the image as True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l-a_5fbYKXXq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from collections import Counter\n",
    "\n",
    "df = pd.read_csv(\"results.csv\") # reads a .csv file into a DataFrame\n",
    "yes_votes = Counter() # holds the number of \"yes\" votes each image receives\n",
    "total_votes = Counter() # holds the number of total votes each image receives\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "  true_images = row['Answer.selected'].split('|') # splits the string into a list with the specified '|' separator\n",
    "  for i in range(1, 13):\n",
    "    url = row[\"Input.image\" + str(i)] \n",
    "    if type(url) is not str: continue # this line skips bad entries in CSV\n",
    "\n",
    "    total_votes[url] += 1 # Add 1 to total_votes when the url received a vote (again, we are try to get the total votes each image receives)\n",
    "\n",
    "    ##### START CODE HERE: Add 1 to yes_votes when the url has a vote (in other words, when the url is in true_image) #####\n",
    "    \n",
    "    ##### END CODE HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFRuBIicFckK"
   },
   "source": [
    "# Train an image classifier to classify weddings\n",
    "\n",
    "We're going to be classifying whether a photo contains a wedding or not. We'll classify an image as a wedding if a majority of the workers said the image represented a wedding.\n",
    "\n",
    "We will train a classifier. It will be trained on Western and NonWestern images, and then assessed on both Western and NonWestern images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RTNS3_A0FSg4"
   },
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "\n",
    "def get_path_from_url(url):\n",
    "  return url.replace('https://s3.amazonaws.com/nets213-hw5/', '')\n",
    "\n",
    "image_urls = list(total_votes.keys()) # remote URLs to the images\n",
    "paths = [get_path_from_url(url) for url in image_urls] # local paths to the image files in the Google Colab files\n",
    "\n",
    "##### START CODE HERE: Write get_label() to return True if the image has a majority of \"yes\" votes, and False otherwise #####\n",
    "def get_label(url): \n",
    "  pass\n",
    "##### END CODE HERE #####\n",
    "\n",
    "##### START CODE HERE: Run get_label() on image_urls to create a list called \"labels\" #####\n",
    "labels = None\n",
    "##### END CODE HERE #####\n",
    "\n",
    "# The code below updates the image URLs of the additional negative Western images that we added to improve training\n",
    "for index in range(0, len(paths)):\n",
    "  if \"random-european-language\" in paths[index]:\n",
    "    sections = paths[index].split('/')\n",
    "    revised_path = \"weddings-european-language/\" + ('/').join(sections[1:])\n",
    "    paths[index] = revised_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0W723MyjD-jA"
   },
   "source": [
    "We have the paths and labels from our compiled **mega_results_cleaned.csv**. As stated before, we wish to assess our classifier firstly on Western images and then secondly on NonWestern images. Thus, we need to separate our paths/labels into two separate sets – the set of Western paths/labels and the set of NonWestern paths/labels.\n",
    "\n",
    "For assessment purposes, we wish to create a test set of Western images and a test set of NonWestern images. These images should not be used to train the classifiers. Thus, we need to split the Western set of images into a training set and a test set, and do the same for the NonWestern set of images.\n",
    "\n",
    "Note that some image URLs that were provided in the MTurk HITs are not valid URLs. In other words, there are several URLs in **paths** that we should ignore. The following code will also do this filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsGb356tGhKN"
   },
   "outputs": [],
   "source": [
    "# Western set of images\n",
    "paths_Western = []\n",
    "labels_Western = []\n",
    "\n",
    "# NonWestern set of images\n",
    "paths_NonWestern = []\n",
    "labels_NonWestern = []\n",
    "\n",
    "##### START CODE HERE: Index through the paths and labels and add to either the Western set or NonWestern set – if a path doesn't contain either \"indian-languages\" or \"european-language\", ignore it #####\n",
    "\n",
    "##### END CODE HERE #####\n",
    "\n",
    "##### START CODE HERE: Split your Western dataset into a training set (80%) and test set (20%), and do the same with your NonWestern dataset – use sklearn.model_selection.train_test_split() with the stratify parameter equal to your labels #####\n",
    "paths_Western_train, paths_Western_test, labels_Western_train, labels_Western_test = None\n",
    "paths_NonWestern_train, paths_NonWestern_test, labels_NonWestern_train, labels_NonWestern_test = None\n",
    "##### END CODE HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lz-iFGW0C8pw"
   },
   "source": [
    "We will now specify the structure of our dataset and then create and train our model. If the given parameter values are used, the training time should be about a minute and the error rate should be around 10-15%. Try to experiment around with different parameters to further tune the model! Examples of potential improvements are data augmentations, additional epochs, and batch sizes. \n",
    "\n",
    "Reference the [documentation](https://docs.fast.ai/vision.data.html#ImageDataLoaders) of ImageDataLoaders to determine the appropriate method to use for the first line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYzODxujaV9N"
   },
   "source": [
    "We will now train our classifier. This classifier will be trained on both our Western training set and NonWestern training set. The Western training set's paths and labels are stored in **paths_Western_train** and **labels_Western_train**. The NonWestern training set's paths and labels are stored in **paths_NonWestern_train** and **labels_NonWestern_train**. We will combine these training sets into **paths_Both_train** and **labels_Both_train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnObNOeQaemY"
   },
   "outputs": [],
   "source": [
    "##### START CODE HERE: Combine the Western training set and the NonWestern training set #####\n",
    "paths_Both_train = None\n",
    "labels_Both_train = None\n",
    "##### END CODE HERE #####\n",
    "\n",
    "##### START CODE HERE: Specify what type of dataset we have and how it's structured with ImageDataLoaders – use a validation percentage of 20%, seed of 42, and item transformation of Resize(224) #####\n",
    "dls_Both = None\n",
    "##### END CODE HERE #####\n",
    "\n",
    "##### START CODE HERE: Create a convolutional neural network called \"classifier_Both\" with cnn_learner() – use an architecture of resnet34, metric of error_rate, and pretrained of True #####\n",
    "classifier_Both = None\n",
    "##### END CODE HERE #####\n",
    "\n",
    "##### START CODE HERE: Fit the model with fine_tune() – use an epoch of 5\n",
    "\n",
    "##### END CODE HERE #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rGPPApFODWq8"
   },
   "source": [
    "Try using a sample wedding image found online with this classifier in order to see if it was successfully trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHA6xFtUDWdk"
   },
   "outputs": [],
   "source": [
    "# Click on the gray \"Upload\" button in order to upload your sample wedding image\n",
    "uploader = widgets.FileUpload()\n",
    "uploader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYT3vHF-DZH5"
   },
   "outputs": [],
   "source": [
    "# The model will predict whether or not your uploaded image is that of a wedding\n",
    "img = PILImage.create(uploader.data[0])\n",
    "is_wedding,_,probs = classifier_Both.predict(img)\n",
    "print(f\"Is this an image of a wedding?: {is_wedding}.\")\n",
    "print(f\"Probability it's a wedding: {probs[1].item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPFFhyIyf00F"
   },
   "source": [
    "# Calculate evaluation metrics for our classifier\n",
    "\n",
    "We will now calculate several evaluation metrics to assess our classifier. Several metrics include precision, recall, and F1-Score. These three metrics can be used to assess how good our classifiers are. An overview can be found [here](https://towardsdatascience.com/a-look-at-precision-recall-and-f1-score-36b5fd0dd3ec). \n",
    "\n",
    "A confusion matrix can help visualize the components used in calculating these metrics:\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"https://miro.medium.com/max/800/1*fxiTNIgOyvAombPJx5KGeA.png\" width=\"350\" />\n",
    "</figure>\n",
    "\n",
    "*   Precision: The ratio of what our model predicted correctly to what our model predicted\n",
    "*   Recall: Ratio of what our model predicted correctly to what the actual labels are\n",
    "*   F1-Score: Harmonic mean of precision and recall\n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"https://miro.medium.com/max/1068/1*EXa-_699fntpUoRjZeqAFQ.jpeg\" />\n",
    "</figure>\n",
    "\n",
    "\n",
    "In order to have a baseline for comparison, we created a classifier pretrained on ImageNet and obtained the following metrics:\n",
    "\n",
    "*   NonWestern Precision from ImageNet classifier: 0.658008658008568\n",
    "*   NonWestern Recall from ImageNet classifier: 0.18225419664268586\n",
    "*   NonWestern F1-Score from ImageNet classifier: 0.28544600938967135\n",
    "*   Western Precision from ImageNet classifier: 0.7463414634146341\n",
    "*   Western Recall from ImageNet classifier: 0.504950495049505\n",
    "*   Western F1-Score from ImageNet classifier: 0.6023622047244095\n",
    "\n",
    "Notice that our ImageNet classifier performs worse on the NonWestern images than on Western images, as seen in the difference in F1-Scores. This result could have been anticipated. [Google researchers](https://research.google/pubs/pub46553/) found that ImageNet “appear[s] to exhibit an observable amerocentric and eurocentric representation bias,” as demonstrated by the distribution of geographically identifiable images in the datasets, with 2/3 of the images from the Western world. \n",
    "\n",
    "<figure align=\"center\">\n",
    "<img src=\"http://crowdsourcing-class.org/images/imagenet_pie_chart.jpg\" />\n",
    "</figure>\n",
    "\n",
    "We will calculate these metrics for our classifier as well. We will have our classifier predict whether or not these images depict a wedding in order to determine our True Positive, True Negative, False Positive, and False Negative values and ultimately calculate the F1-Scores.\n",
    "\n",
    "We will use the predictions of your classifier on **paths_Western_test** with **labels_Western_test** to assess the TP, TN, FP, and FN values on Western images. We will use the predictions of your classifier on **paths_NonWestern_test** with **labels_NonWestern_test** to assess the TP, TN, FP, and FN values on NonWestern images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlrRZSznzuee"
   },
   "source": [
    "# Calculate evaluation metrics for your classifier trained on both Western and NonWestern images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F2smgH4RHprV"
   },
   "source": [
    "Firstly, calculate the precision, recall, and F1-score for your classifier trained on both Western and NonWestern images and assessed with Western images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 16
    },
    "id": "NuslWySIzw4t",
    "outputId": "27f8dc6e-5118-436a-d0c4-0381b9e18828"
   },
   "outputs": [],
   "source": [
    "predictions_Both_Western = []\n",
    "\n",
    "##### START CODE HERE: Create a list called predictions_Both_Western that stores all predictions for the Western images in paths_Western_test #####\n",
    "predictions_Both_Western = None\n",
    "##### END CODE HERE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x54GaBwWH6Wo"
   },
   "outputs": [],
   "source": [
    "##### START CODE HERE: Calculate the number of True Positives, True Negatives, False Positives, and False Negatives for your classifier_Both on the Western images #####\n",
    "TP_Both_Western = None\n",
    "TN_Both_Western = None\n",
    "FP_Both_Western = None\n",
    "FN_Both_Western = None\n",
    "##### END CODE HERE #####\n",
    "\n",
    "##### START CODE HERE: Calculate precision, recall, and F1-score for your classifier_Both on the Western images #####\n",
    "precision_Both_Western = None\n",
    "recall_Both_Western = None\n",
    "f1_score_Both_Western = None\n",
    "##### END CODE HERE #####\n",
    "\n",
    "# Display your confusion matrix, precision, recall, and F1-score for your classifier_Both on the Western images\n",
    "confusion_matrix_data = [(TP_Both_Western, FP_Both_Western) , (FN_Both_Western, TN_Both_Western)]\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix_data, columns = ['Actual Wedding' , 'Actual Non-Wedding'], index=['Predicted Wedding', 'Predicted Non-Wedding'])\n",
    "print(\"Confusion Matrix:\")\n",
    "display(confusion_matrix)\n",
    "print(\"Western Precision from your classifier trained on both Western and NonWestern images: \" + str(precision_Both_Western))\n",
    "print(\"Western Recall from your classifier trained on both Western and NonWestern images: \" + str(recall_Both_Western))\n",
    "print(\"Western F1-Score from your classifier trained on both Western and NonWestern images: \" + str(f1_score_Both_Western))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3-ttYy_IQyQ"
   },
   "source": [
    "Secondly, calculate the precision, recall, and F1-score for your classifier trained on both Western and NonWestern images and assessed with NonWestern images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 16
    },
    "id": "z1v5NWcgIVJX",
    "outputId": "052b9712-8291-401f-d125-ad6eed86e689"
   },
   "outputs": [],
   "source": [
    "predictions_Both_NonWestern = []\n",
    "\n",
    "##### START CODE HERE: Create a list called predictions_Both_NonWestern that stores all predictions for the NonWestern images in paths_NonWestern_test #####\n",
    "predictions_Both_NonWestern = None\n",
    "##### END CODE HERE #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZzW0cAdbIgje"
   },
   "outputs": [],
   "source": [
    "##### START CODE HERE: Calculate the number of True Positives, True Negatives, False Positives, and False Negatives for your classifier_Both on the NonWestern images #####\n",
    "TP_Both_NonWestern = None\n",
    "TN_Both_NonWestern = None\n",
    "FP_Both_NonWestern = None\n",
    "FN_Both_NonWestern = None\n",
    "##### END CODE HERE #####\n",
    "\n",
    "##### START CODE HERE: Calculate precision, recall, and F1-score for your classifier_Both on the NonWestern images #####\n",
    "precision_Both_NonWestern = None\n",
    "recall_Both_NonWestern = None\n",
    "f1_score_Both_NonWestern = None\n",
    "##### END CODE HERE #####\n",
    "\n",
    "# Display your confusion matrix, precision, recall, and F1-score for your classifier_Both on the NonWestern images\n",
    "confusion_matrix_data = [(TP_Both_NonWestern, FP_Both_NonWestern) , (FN_Both_NonWestern, TN_Both_NonWestern)]\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix_data, columns = ['Actual Wedding' , 'Actual Non-Wedding'], index=['Predicted Wedding', 'Predicted Non-Wedding'])\n",
    "print(\"Confusion Matrix:\")\n",
    "display(confusion_matrix)\n",
    "print(\"NonWestern Precision from your classifier trained on both Western and NonWestern images: \" + str(precision_Both_NonWestern))\n",
    "print(\"NonWestern Recall from your classifier trained on both Western and NonWestern images: \" + str(recall_Both_NonWestern))\n",
    "print(\"NonWestern F1-Score from your classifier trained on both Western and NonWestern images: \" + str(f1_score_Both_NonWestern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDRS6NPpnc7K"
   },
   "source": [
    "# Visual validation on your classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UfAjI7KInd8F"
   },
   "outputs": [],
   "source": [
    "print(\"------------------------------------------------------------------\")\n",
    "display(IPython.display.Image(classifier_Western_false_negative_images[0]))\n",
    "print(\"Classifier Predicted:\", \"wedding\" if 'True' == str(classifier_Both.predict(classifier_Western_false_negative_images[0])[0]) else \"not-wedding\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "display(IPython.display.Image(classifier_Western_false_negative_images[1]))\n",
    "print(\"Classifier Predicted:\", \"wedding\" if 'True' == str(classifier_Both.predict(classifier_Western_false_negative_images[1])[0]) else \"not-wedding\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "display(IPython.display.Image(classifier_Western_false_negative_images[2]))\n",
    "print(\"Classifier Predicted:\", \"wedding\" if 'True' == str(classifier_Both.predict(classifier_Western_false_negative_images[2])[0]) else \"not-wedding\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "display(IPython.display.Image(classifier_Western_false_negative_images[3]))\n",
    "print(\"Classifier Predicted:\", \"wedding\" if 'True' == str(classifier_Both.predict(classifier_Western_false_negative_images[3])[0]) else \"not-wedding\")\n",
    "print(\"------------------------------------------------------------------\")\n",
    "display(IPython.display.Image(classifier_Western_false_negative_images[4]))\n",
    "print(\"------------------------------------------------------------------\")\n",
    "print(\"Classifier Predicted:\", \"wedding\" if 'True' == str(classifier_Both.predict(classifier_Western_false_negative_images[4])[0]) else \"not-wedding\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Train a Classifier_V2_STUDENT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
