{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb374666",
   "metadata": {},
   "source": [
    "<center>\n",
    "In God We Trust\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0370e29f",
   "metadata": {},
   "source": [
    "# CE417: Artificial Intelligence\n",
    "\n",
    "Dr. Mahdiyeh Soleymani Baghshah, Associate Professor\n",
    "\n",
    "Computer Engineering Department,\n",
    "Sharif University of Technology,\n",
    "Tehran, Tehran, Iran\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae94a10",
   "metadata": {},
   "source": [
    "### Name: Parham Bateni\n",
    "### Student number: 99105294"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74463d06-3320-48b6-b189-c1eb3819ef06",
   "metadata": {},
   "source": [
    "# MLP MNIST CLassifier (20 Points)\n",
    "\n",
    "Corresponding TA: Parham Saremi, Aryan Ahadinia\n",
    "\n",
    "In this question we aim to implement dense neural network from base and train a model for MNIST classification with that. MNIST is a set of 28 by 28 pixels images of handwritten digits. In this problem, you are going to implement neural network using NumPy. You are NOT PERMITTED to use any libraries except NumPy.\n",
    "\n",
    "**Required features of the model**:\n",
    "Your implementation should be parametrized and dynamic meaning that your MLP must be instantiated with any number of layers and dimension size for the layers. \n",
    "\n",
    "**NOTE**: Most of your score is for your implementation and the existence and the quality of your results (Final numbers doesn't matter that much but your model's ability to learn is important).\n",
    "\n",
    "**NOTE**: your module's logic must be implemented in NumPy without any python's for loops (or while loops :)). However, you can use for loops for iterating on different layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f28c2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are denied to add any other packages.\n",
    "\n",
    "from abc import abstractmethod\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df87d8",
   "metadata": {},
   "source": [
    "## Loss Function (3 Points)\n",
    "\n",
    "Loss function is one of the most important part of most of the ML methods. In this part, we want to implement loss function. We implemented an abstract class for loss function `LossFunction`. In following cells, you have to implement Mean Squared Error, Mean Absolute Error and Cross Entropy Loss. You have to implement `forward` and `backward` methods.\n",
    "\n",
    "Hint: You must save some variables as a field in the class instance in `forward` call. You are going to need them in ‚Äç`backward` call to calculate gradient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a553f79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunction:\n",
    "    @abstractmethod\n",
    "    def forward(self, y_hat, y):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self,upstream):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d6405e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanSquaredError(LossFunction):\n",
    "    def forward(self, y_hat, y):\n",
    "        # Hint: Saving some fields for backward\n",
    "        # y and y_hat are batch matrices each row corresponding to one sample\n",
    "        self.m = y.shape[1]\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        return sum(np.mean(np.power(y-y_hat,2),axis=1))\n",
    "\n",
    "    def backward(self,upstream):\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        # y and y_hat are batch matrices each row corresponding to one sample\n",
    "        # upstream : gradient of loss w.r.t model output\n",
    "        return 2/self.m*(self.y_hat-self.y)*upstream\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Mean Squared Error\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e4ba1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.5\n",
      "[[ 0. -1.]\n",
      " [ 3.  0.]\n",
      " [-1.  2.]]\n"
     ]
    }
   ],
   "source": [
    "# Test MeanSquaredError\n",
    "test=MeanSquaredError()\n",
    "y=np.array([[0,2],[-1,1],[1,0]])\n",
    "y_hat=np.array([[0,1],[2,1],[0,2]])\n",
    "print(test.forward(y_hat,y))\n",
    "print(test.backward(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fda0f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanAbsoluteError(LossFunction):\n",
    "    def forward(self, y_hat, y):\n",
    "        self.m = y.shape[1]\n",
    "        self.y_hat = y_hat\n",
    "        self.y = y\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        # y and y_hat are batch matrices each row corresponding to one sample\n",
    "        return sum(np.mean(np.abs(y_hat-y),axis=1))\n",
    "\n",
    "    def backward(self,upstream):\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        # y and y_hat are batch matrices each row corresponding to one sample\n",
    "        return -np.sign(self.y-self.y_hat)/self.m*upstream\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Mean Absolute Error\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e8d27a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n",
      "[[ 0.  -0.5]\n",
      " [ 0.5  0. ]\n",
      " [-0.5  0.5]]\n"
     ]
    }
   ],
   "source": [
    "# Test MeanAbsoluteError\n",
    "test=MeanAbsoluteError()\n",
    "y=np.array([[0,2],[-1,1],[1,0]])\n",
    "y_hat=np.array([[0,1],[2,1],[0,2]])\n",
    "print(test.forward(y_hat,y))\n",
    "print(test.backward(np.array(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "38afe418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CrossEntropyLoss(LossFunction):\n",
    "#     def forward(self, y_hat, y):\n",
    "#         self.y_hat = y_hat\n",
    "#         self.y = y\n",
    "#         ###################################\n",
    "#         ############ CODE HERE ############\n",
    "#         ###################################\n",
    "#         # y and y_hat are batch matrices each row corresponding to one sample\n",
    "#         # We add epsilon in case x is zero in the log(x)\n",
    "#         targets_matrix=np.zeros_like(y_hat)\n",
    "#         for i in range(len(y)):\n",
    "#             targets_matrix[i][y[i]]=1\n",
    "#         self.targets_matrix=targets_matrix\n",
    "#         epsilon=10e-10\n",
    "#         return -sum(np.diag(np.dot(np.log(y_hat+epsilon),targets_matrix.T)))\n",
    "\n",
    "#     def backward(self,upstream):\n",
    "#         ###################################\n",
    "#         ############ CODE HERE ############\n",
    "#         ###################################\n",
    "#         # y and y_hat are batch matrices each row corresponding to one sample\n",
    "#         epsilon=10e-10\n",
    "# #         return -self.y*upstream\n",
    "#         return -self.targets_matrix/(self.y_hat+epsilon)*upstream\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "9189bfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(LossFunction):\n",
    "    def forward(self, logprobs, targets):\n",
    "        \"\"\"\n",
    "        Calculate cross entropy of inputs.\n",
    "\n",
    "        Args:\n",
    "            probs : matrix of probabilities with shape (b,n)\n",
    "            targets : list of samples classes with shape (b,)\n",
    "\n",
    "        Returns:\n",
    "            y : cross entropy loss\n",
    "        \"\"\"\n",
    "        targets_matrix=np.zeros_like(logprobs)\n",
    "        for i in range(len(targets)):\n",
    "            targets_matrix[i][targets[i]]=1\n",
    "        self.targets_matrix=targets_matrix\n",
    "        y=-logprobs.dot(targets_matrix.T).diagonal()\n",
    "        \n",
    "        return np.sum(y)\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        \"\"\"\n",
    "        Calculate gradient of loss w.r.t module input and save them in grads.\n",
    "\n",
    "        Args:\n",
    "            upstream : gradient of loss w.r.t module output (loss)\n",
    "        \"\"\"\n",
    "        self.grad_x = -self.targets_matrix*upstream\n",
    "        return self.grad_x\n",
    "    def __repr__(self):\n",
    "        return \"Cross Entropy Loss\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "0f9fc71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_gradient_ce(ce, h=0.0001):\n",
    "#     np.random.seed(123123)\n",
    "#     target = np.random.randint(5, size=10)\n",
    "#     probs = np.random.random(size=(10, 5))\n",
    "#     upstream = 1\n",
    "\n",
    "#     new_probs = probs + h\n",
    "#     new_val = ce.forward(new_probs, target)\n",
    "#     old_val = ce.forward(probs, target)\n",
    "#     delta_output = new_val - old_val\n",
    "#     delta_loss_indirect = np.sum(delta_output * upstream)\n",
    "#     grad_x=ce.backward(upstream)\n",
    "#     delta_loss_direct = np.sum(h * grad_x)\n",
    "\n",
    "#     print(f'Gradient of loss w.r.t output:\\n{upstream}')\n",
    "#     print(f\"Gradient of loss w.r.t input:\\n{grad_x}\")\n",
    "#     print(f'Relative error of delta-loss:\\n{rel_error(delta_loss_indirect, delta_loss_direct)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "743bd624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ce = CrossEntropyLoss()\n",
    "# check_gradient_ce(ce, h=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "298c3e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.6\n",
      "[[-0. -0. -1.]\n",
      " [-0. -1. -0.]\n",
      " [-1. -0. -0.]]\n"
     ]
    }
   ],
   "source": [
    "test=CrossEntropyLoss()\n",
    "y=np.array([2,1,0])\n",
    "y_hat=np.array([[0.1,0.2,0.7],[0.1,0.9,0],[1,0,0]])\n",
    "print(test.forward(y_hat,y))\n",
    "print(test.backward(np.array(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab75c2f",
   "metadata": {},
   "source": [
    "## Activation Functions (3 Points)\n",
    "\n",
    "Now we are going to implement some activation functions. We will implement the following activation functions: Sigmoid, Leaky ReLU, and Softmax. You have to implement both forward and backward methods for this class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50305a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunction:\n",
    "    @abstractmethod\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def backward(self,upstream):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "abbed3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(ActivationFunction):\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        # x is a batch of data and is in shape (batch_size*784)\n",
    "        self.sig=1/(1+np.exp(-x))\n",
    "        return self.sig\n",
    "\n",
    "    def backward(self,upstream):\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        # x is a batch of data and is in shape (batch_size*784)\n",
    "        # upstream: [[d_L/d_y11, d_L,d_y12, ..., d_L/d_y1m], ...]\n",
    "        return self.sig*(1-self.sig)*upstream\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Sigmoid\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "c420da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5        0.73105858]\n",
      " [0.88079708 0.73105858]\n",
      " [0.5        0.88079708]]\n",
      "[[ 0.         -0.09830597]\n",
      " [ 0.05249679  0.        ]\n",
      " [-0.125       0.05249679]]\n"
     ]
    }
   ],
   "source": [
    "# Test Sigmoid\n",
    "test=Sigmoid()\n",
    "y=np.array([[0,1],[2,1],[0,2]])\n",
    "print(test.forward(y))\n",
    "upstream=np.array([[ 0.,-0.5],[ 0.5,0. ],[-0.5,0.5]])\n",
    "print(test.backward(upstream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "a1d9f11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeakyReLU(ActivationFunction):\n",
    "    def __init__(self, alpha=0.01):\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        # x is a batch of data and is in shape (batch_size*784)\n",
    "        return np.maximum(x,self.alpha*x)\n",
    "\n",
    "    def backward(self,upstream):\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        # x is a batch of data and is in shape (batch_size*784)\n",
    "        # upstream: [[d_L/d_y11, d_L,d_y12, ..., d_L/d_y1m], ...]\n",
    "        grad_x=upstream.copy()\n",
    "        grad_x[self.x<0]=self.alpha*grad_x[self.x<0]\n",
    "        return grad_x\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"Leaky ReLU\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "09e04c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    2.  ]\n",
      " [-0.01  1.  ]\n",
      " [ 1.   -0.02]]\n",
      "[[ 0.    -0.5  ]\n",
      " [ 0.005  0.   ]\n",
      " [-0.5    0.005]]\n"
     ]
    }
   ],
   "source": [
    "# Test LeakyReLU\n",
    "test=LeakyReLU()\n",
    "y=np.array([[0,2],[-1,1],[1,-2]])\n",
    "print(test.forward(y))\n",
    "upstream=np.array([[ 0.,-0.5],[ 0.5,0. ],[-0.5,0.5]])\n",
    "print(test.backward(upstream))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "9b5578de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "[[-2.40760596 -1.40760596 -0.40760596]\n",
      " [-2.40760596 -1.40760596 -0.40760596]]\n"
     ]
    }
   ],
   "source": [
    "def logsumexp(array, axis=1):\n",
    "    \"\"\"\n",
    "    calculate log(sum(exp(array))) using np.logaddexp\n",
    "\n",
    "    Args:\n",
    "        array : input array\n",
    "        axis : reduce axis, 1 means columns and 0 means rows\n",
    "    \"\"\"\n",
    "    if len(array)==1:\n",
    "        return array\n",
    "    t_array=array.copy() if axis==1 else array.T.copy()\n",
    "    sums=[]\n",
    "    for row in t_array:\n",
    "        s=float('-inf')\n",
    "        for num in row:\n",
    "            s=np.logaddexp(s,num)\n",
    "        sums.append(s)\n",
    "    t_array=t_array-np.array(sums).reshape(-1,1).dot(np.ones((1,t_array.shape[1])))\n",
    "    return t_array if axis==1 else t_array.T\n",
    "test=np.array([[1,2,3],[4,5,6]])\n",
    "print(test)\n",
    "print(logsumexp(test,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "63300eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogSoftMax(ActivationFunction):\n",
    "    def __init__(self):\n",
    "        self.cache = dict()\n",
    "        self.grads = dict()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        get x and calculate softmax of that.\n",
    "\n",
    "        Args:\n",
    "            x : batch of data with shape (b,m)\n",
    "\n",
    "        Returns:\n",
    "            y : log softmax of x with shape (b,m)\n",
    "        \"\"\"\n",
    "        y = logsumexp(x,axis=1)\n",
    "        self.cache['y']=y\n",
    "        return y\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        \"\"\"\n",
    "        calculate gradient of loss w.r.t module input and save that in grads.\n",
    "\n",
    "        Args:\n",
    "            upstream : gradient of loss w.r.t module output with sahpe (b,m)\n",
    "        \"\"\"\n",
    "        # grad_x: [[d_L/d_x11, d_L/d_x12, ..., d_L/d_x1m], ...]\n",
    "        # upstream: [[d_L/d_y11, d_L,d_y12, ..., d_L/d_y1m], ...]\n",
    "        # z1=log(sum(exp(x11),exp(x12),...,exp(x1m))), ..., zm=log(sum(xb1,xb2,...,xbm))\n",
    "        # y=[[x11-z1,x12-z1,...,x1m-z1],..., [xb1-zb,xb2-zb,...,xbm-zb]] = [[y11,y12,...y1m],...,[yb1,yb2,...,ybm]]\n",
    "        # -> d_L/d_x11=d_L/d_y11 * d_y11/d_x11 + d_L/d_y12 * d_y12/d_x11 + ...\n",
    "        # -> d_L/d_x11=d_L/d_y11 * (1-exp(x11)/sum(exp(x11),...,exp(x1m))) - d_L/d_y12 * exp(x11)/sum(exp(x11),...,exp(x1m)) - ...\n",
    "        # -> d_L/d_x11=-(exp(x11)/sum(exp(x11),...,exp(x1m)))*(d_L/d_y11+...d_L/d_y1m) + d_L/d_y11\n",
    "        # -> d_L/d_x11=-(exp(x11-z1))*(d_L/d_y11+...+d_L/d_y1m) + d_L/d_y11\n",
    "#         print('upstream in logsoftmax is:\\n', upstream)\n",
    "        self.grad_x = -np.exp(self.cache['y'])*np.sum(upstream,axis=1).reshape(-1,1)+upstream\n",
    "        return self.grad_x\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"LogSoftMax\"\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "ca98ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Softmax(ActivationFunction):\n",
    "#     def forward(self, x):\n",
    "#         self.x = x\n",
    "#         ###################################\n",
    "#         ############ CODE HERE ############\n",
    "#         ###################################\n",
    "#         # x is a batch of data and is in shape (batch_size*784)\n",
    "#         self.sof=np.exp(x)/(np.dot(np.sum(np.exp(x),axis=1).reshape((-1,1)),np.ones((1,x.shape[1]))))\n",
    "#         return self.sof\n",
    "\n",
    "#     def backward(self,upstream):\n",
    "#         ###################################\n",
    "#         ############ CODE HERE ############\n",
    "#         ###################################\n",
    "#         # x is a batch of data and is in shape (batch_size*784)\n",
    "#         # upstream: [[d_L/d_y11, d_L,d_y12, ..., d_L/d_y1m], ...]\n",
    "#         # y=[[y11,y12,...y1m],...,[yb1,yb2,...,ybm]]\n",
    "#         # -> d_L/d_x11=d_L/d_y11 * d_y11/d_x11 + d_L/d_y12 * d_y12/d_x11 + ...\n",
    "#         # -> d_L/d_x11=d_L/d_y11 * (1-exp(x11)/sum(exp(x11),...,exp(x1m))) - d_L/d_y12 * exp(x11)/sum(exp(x11),...,exp(x1m)) - ...\n",
    "#         # -> d_L/d_x11=-(exp(x11)/sum(exp(x11),...,exp(x1m)))*(d_L/d_y11+...d_L/d_y1m) + d_L/d_y11\n",
    "#         grad_x = -self.sof*np.sum(upstream,axis=1).reshape(-1,1)+upstream\n",
    "#         return grad_x\n",
    "#     def __repr__(self) -> str:\n",
    "#         return \"Softmax\"\n",
    "\n",
    "#     def __str__(self) -> str:\n",
    "#         return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "835d9e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def rel_error(x, y):\n",
    "#     return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "# def check_gradient_softmax(sm, h=0.0001):\n",
    "#     np.random.seed(321321)\n",
    "#     x = np.random.random(size=(10, 5))\n",
    "#     upstream = np.random.random(size=(10, 5))\n",
    "\n",
    "#     new_x = x + h\n",
    "#     new_val = sm.forward(new_x)\n",
    "#     old_val = sm.forward(x)\n",
    "#     delta_output = new_val - old_val\n",
    "#     delta_loss_indirect = np.sum(delta_output * upstream)\n",
    "#     grad_x=sm.backward(upstream)\n",
    "#     delta_loss_direct = np.sum(h * grad_x)\n",
    "#     print(f'Gradient of loss w.r.t output:\\n{upstream}')\n",
    "#     print(f\"Gradient of loss w.r.t input:\\n{grad_x}\")\n",
    "#     print(f'Relative error of delta-loss:\\n{rel_error(delta_loss_indirect, delta_loss_direct)}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "ce5ae230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sm=LogSoftMax()\n",
    "# check_gradient_softmax(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "55d90ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.11920292 0.88079708]\n",
      " [0.11920292 0.88079708]\n",
      " [0.73105858 0.26894142]]\n",
      "[[ 0.05960146 -0.05960146]\n",
      " [ 0.44039854 -0.44039854]\n",
      " [-0.5         0.5       ]]\n"
     ]
    }
   ],
   "source": [
    "# Test Softmax\n",
    "test=Softmax()\n",
    "y=np.array([[0,2],[-1,1],[1,0]])\n",
    "print(test.forward(y))\n",
    "upstream=np.array([[ 0.,-0.5],[ 0.5,0. ],[-0.5,0.5]])\n",
    "print(test.backward(upstream))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c87086",
   "metadata": {},
   "source": [
    "## Dense Layer (4 Points)\n",
    "\n",
    "Now it's the time to implement a single dense layer. Each dense layer has an an input vector and output vector size and an activation function. You have to implement two methods: `forward` and `backward`.\n",
    "\n",
    "Hint: `backward` method get gradient of previous backward step as input, it has to calculate gradient of weights and biases and save them in the class instance and return gradient of this step as output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "f6cecf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, activation):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation = activation\n",
    "\n",
    "        self.w = np.random.randn(output_size, input_size) * np.sqrt(2 / input_size)\n",
    "        self.b = np.zeros((output_size, 1))\n",
    "\n",
    "        # Leave these fields unchanged, Use them in forward and backward\n",
    "        self.z = None  # Output of transformation\n",
    "        self.a = None  # Output of activation\n",
    "        self.dw = None  # Gradient of weights\n",
    "        self.db = None  # Gradient of biases\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        self.x=x\n",
    "        self.z=x@self.w.T+np.ones((x.shape[0],1)).dot(self.b.T)\n",
    "        self.a=self.activation.forward(self.z)\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, upstream):\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        upstream=self.activation.backward(upstream)\n",
    "        self.dw = upstream.T.dot(self.x)\n",
    "        self.db= upstream.T.dot(np.ones((upstream.shape[0],1)))\n",
    "        self.dx = upstream.dot(self.w)\n",
    "        return self.dx\n",
    "\n",
    "    def update(self, lr):\n",
    "        self.w -= lr * self.dw\n",
    "        self.b -= lr * self.db\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"Dense {self.input_size} -> {self.output_size} with {self.activation}\"\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "e91ade6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy\n",
    "# def check_gradient_linear(linear, h=0.00001):\n",
    "#     np.random.seed(121212)\n",
    "#     x = np.random.normal(size=(10, linear.input_size))\n",
    "#     upstream = np.random.random(size=(10, linear.output_size))\n",
    "\n",
    "#     new_x = x + h\n",
    "#     new_w = linear.w + h\n",
    "#     new_b = linear.b + h\n",
    "#     new_linear = copy.deepcopy(linear)\n",
    "#     new_linear.w = new_w\n",
    "#     new_linear.b = new_b\n",
    "#     new_val = new_linear.forward(new_x)\n",
    "#     old_val = linear.forward(x)\n",
    "#     delta_output = new_val - old_val\n",
    "#     delta_loss_indirect = np.sum(delta_output * upstream)\n",
    "\n",
    "#     linear.backward(upstream)\n",
    "#     delta_loss_direct = np.sum(h * linear.dx)\n",
    "#     delta_loss_direct += np.sum(h * linear.dw)\n",
    "#     delta_loss_direct += np.sum(h * linear.db)\n",
    "\n",
    "#     print(f'Gradient of loss w.r.t output:\\n{upstream}')\n",
    "#     print(f\"Gradient of loss w.r.t input:\\n{linear.dx}\")\n",
    "#     print(f\"Gradient of loss w.r.t W:\\n{linear.dw}\")\n",
    "#     print(f\"Gradient of loss w.r.t b:\\n{linear.db}\")\n",
    "#     print(f'Relative error of delta-loss (for linear unit):\\n{rel_error(delta_loss_indirect, delta_loss_direct)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "14179531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear = Layer(5, 10, LeakyReLU())\n",
    "# check_gradient_linear(linear, h=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7767b08",
   "metadata": {},
   "source": [
    "## Data (2 Points)\n",
    "\n",
    "In cells below, we have implemented `Data` and `DataLoader` classes. Now you have to use these classes and load MNIST dataset on them. You have to normalize value of each pixel in way that it goes to interval of [0-1] and after that, shift the data in way that it get zero mean. You have to download MNIST data and you are not permitted to use libraries to load that data. You have to create two datasets: Train and Test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "9256fd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(zip(self.x, self.y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "dafa1aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, data, batch_size):\n",
    "        self.data = data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.data), self.batch_size):\n",
    "            yield self.data[i : i + self.batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "957ba5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A piece of code for converting the files into csv\n",
    "# def convert(imgf, labelf, outf, n):\n",
    "#     f = open(imgf, \"rb\")\n",
    "#     o = open(outf, \"w\")\n",
    "#     l = open(labelf, \"rb\")\n",
    "\n",
    "#     f.read(16)\n",
    "#     l.read(8)\n",
    "#     images = []\n",
    "\n",
    "#     for i in range(n):\n",
    "#         image = [ord(l.read(1))]\n",
    "#         for j in range(28*28):\n",
    "#             image.append(ord(f.read(1)))\n",
    "#         images.append(image)\n",
    "\n",
    "#     for image in images:\n",
    "#         o.write(\",\".join(str(pix) for pix in image)+\"\\n\")\n",
    "#     f.close()\n",
    "#     o.close()\n",
    "#     l.close()\n",
    "\n",
    "# convert(\"train-images.idx3-ubyte\", \"train-labels.idx1-ubyte\",\n",
    "#         \"mnist_train.csv\", 60000)\n",
    "# convert(\"t10k-images.idx3-ubyte\", \"t10k-labels.idx1-ubyte\",\n",
    "#         \"mnist_test.csv\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f2d9392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x shape is (60000, 784)\n",
      "Mean each train image pixels is zero\n",
      "test_x shape is (10000, 784)\n",
      "Mean each test image pixels is zero\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "########## Load MNIST Dataset, CODE HERE ##########\n",
    "###################################################\n",
    "#The MNIST data is now in csv format\n",
    "import pandas as pd\n",
    "train_df=pd.read_csv('mnist_train.csv',header=None)\n",
    "train_y=train_df[0].to_numpy()\n",
    "train_df.drop(0,axis=1,inplace=True)\n",
    "train_x=train_df.to_numpy().reshape((-1,28*28))\n",
    "print(f'train_x shape is {train_x.shape}')\n",
    "\n",
    "#Normalize\n",
    "train_x=np.divide(train_x,np.max(train_x,axis=1).reshape(-1,1)*np.ones((train_x.shape[0],train_x.shape[1])),casting='safe')\n",
    "train_x-=np.mean(train_x,axis=1).reshape(-1,1)*np.ones((train_x.shape[0],train_x.shape[1]))\n",
    "\n",
    "assert np.allclose(np.mean(train_x,axis=1),np.zeros(train_x.shape[0]))\n",
    "print('Mean each train image pixels is zero')\n",
    "\n",
    "test_df=pd.read_csv('mnist_test.csv',header=None)\n",
    "test_y=test_df[0].to_numpy()\n",
    "test_df.drop(0,axis=1,inplace=True)\n",
    "test_x=test_df.to_numpy().reshape((-1,28*28))\n",
    "print(f'test_x shape is {test_x.shape}')\n",
    "\n",
    "#Normalize\n",
    "test_x=np.divide(test_x,np.max(test_x,axis=1).reshape(-1,1)*np.ones((test_x.shape[0],test_x.shape[1])),casting='safe')\n",
    "test_x-=np.mean(test_x,axis=1).reshape(-1,1)*np.ones((test_x.shape[0],test_x.shape[1]))\n",
    "\n",
    "assert np.allclose(np.mean(test_x,axis=1),np.zeros(test_x.shape[0]))\n",
    "print('Mean each test image pixels is zero')\n",
    "\n",
    "train_data=Data(train_x,train_y)\n",
    "test_data=Data(test_x,test_y)\n",
    "batch_size=60\n",
    "train_data_loader=DataLoader(train_data,batch_size)\n",
    "test_data_loader=DataLoader(test_data,batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1e39819f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60, 784)\n",
      "(60,)\n"
     ]
    }
   ],
   "source": [
    "for x in train_data_loader:\n",
    "    print(x[0].shape)\n",
    "    print(x[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3198552",
   "metadata": {},
   "source": [
    "## Neural Network (4 Points)\n",
    "\n",
    "Now we are going to implement a neural network. You have to implement `forward`, `backward`, and `fit` functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "386c2dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, layers, loss):\n",
    "        self.layers = layers\n",
    "        self.loss = loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        out=None\n",
    "        self.x_shape=x.shape\n",
    "        for layer in self.layers:\n",
    "            if out is None:out=layer.forward(x)\n",
    "            else:out=layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def backward(self,upstream):\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        copy_layers=self.layers.copy()\n",
    "        copy_layers.reverse()\n",
    "        for layer in copy_layers:\n",
    "            upstream=layer.backward(upstream)\n",
    "        \n",
    "\n",
    "    def update(self, lr):\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr)\n",
    "\n",
    "    def fit(self, data_loader, epochs, lr, batch_size=1):\n",
    "        ###################################\n",
    "        ############ CODE HERE ############\n",
    "        ###################################\n",
    "        data_loader.batch_size=batch_size\n",
    "        train_losses=[]\n",
    "        for epoch in range(epochs):\n",
    "            for i,batch in enumerate(data_loader):\n",
    "                x_batch=batch[0]\n",
    "                y_batch=batch[1]\n",
    "\n",
    "                y_pred=self.predict(x_batch)\n",
    "                loss_train=self.loss.forward(y_pred,y_batch)\n",
    "                train_losses.append(loss_train)\n",
    "                self.backward(self.loss.backward(np.ones((batch_size,1))))\n",
    "                self.update(lr)\n",
    "                if i%100==0:\n",
    "                    print(f'Batch: {i}/{len(data_loader.data)/data_loader.batch_size}, Loss: {loss_train}')\n",
    "        return train_losses\n",
    "                \n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return \"\\n\".join([str(layer) for layer in self.layers])\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return self.__repr__()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c086c4a",
   "metadata": {},
   "source": [
    "## Training Model (2 Points)\n",
    "\n",
    "Now, use your neural network to train a model to predict class of MNIST images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "2f71ef53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense 784 -> 256 with Leaky ReLU\n",
      "Dense 256 -> 64 with Sigmoid\n",
      "Dense 64 -> 64 with Sigmoid\n",
      "Dense 64 -> 10 with LogSoftMax\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "########## Train the Model #############\n",
    "########################################\n",
    "layers=[Layer(784,256,LeakyReLU()),\n",
    "        Layer(256,64,Sigmoid()),\n",
    "        Layer(64,64,Sigmoid()),\n",
    "        Layer(64,10,LogSoftMax())]\n",
    "loss=CrossEntropyLoss()\n",
    "nn=NeuralNetwork(layers,loss)\n",
    "print(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "28981937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/600.0, Loss: 260.6513868015703\n",
      "Batch: 100/600.0, Loss: 52.06486016149007\n",
      "Batch: 200/600.0, Loss: 75.40647879367525\n",
      "Batch: 300/600.0, Loss: 23.229322025001853\n",
      "Batch: 400/600.0, Loss: 29.223361181116992\n",
      "Batch: 500/600.0, Loss: 15.566231136156322\n",
      "Batch: 0/600.0, Loss: 15.680390443702349\n",
      "Batch: 100/600.0, Loss: 16.038926726550297\n",
      "Batch: 200/600.0, Loss: 15.320095018658426\n",
      "Batch: 300/600.0, Loss: 17.484458716373787\n",
      "Batch: 400/600.0, Loss: 11.483690618802038\n",
      "Batch: 500/600.0, Loss: 8.722614332666517\n",
      "Batch: 0/600.0, Loss: 8.923702280879628\n",
      "Batch: 100/600.0, Loss: 11.581962503794033\n",
      "Batch: 200/600.0, Loss: 9.87294230673329\n",
      "Batch: 300/600.0, Loss: 10.580531159062716\n",
      "Batch: 400/600.0, Loss: 7.637475199915145\n",
      "Batch: 500/600.0, Loss: 5.589312761638139\n"
     ]
    }
   ],
   "source": [
    "epochs=3\n",
    "lr=0.01\n",
    "train_losses=nn.fit(train_data_loader,epochs,lr,batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b236c25",
   "metadata": {},
   "source": [
    "## Loss Curve (1 Points)\n",
    "\n",
    "Plot curve of loss in each epoch. It should be an smooth descending function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "4408799a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1ac108388d0>]"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################\n",
    "########## Plot loss curve #############\n",
    "########################################\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Loss curve')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch/Batch')\n",
    "plt.plot(np.arange(0,len(train_losses)),train_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524cc034",
   "metadata": {},
   "source": [
    "## Evaluation (1 Points)\n",
    "\n",
    "Now evaluate your model and measure how accurate is your model on both train and test datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "8720070c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train is: 0.9819\n",
      "Accuracy test is: 0.9722\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_hat, y):\n",
    "    ###################################\n",
    "    ############ CODE HERE ############\n",
    "    ###################################\n",
    "    return np.mean(np.argmax(y_hat,axis=1)==y)\n",
    "# y_hat=np.array([[0.2,0.5,0.3],[0.6,0.1,0.3]])\n",
    "# y=np.array([1,0])\n",
    "# accuracy(y_hat,y)\n",
    "\n",
    "\n",
    "########################################\n",
    "########## Calculate accuracy ##########\n",
    "########################################\n",
    "print(f'Accuracy train is: {accuracy(nn.predict(train_x),train_y)}')\n",
    "print(f'Accuracy test is: {accuracy(nn.predict(test_x),test_y)}')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46381811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
