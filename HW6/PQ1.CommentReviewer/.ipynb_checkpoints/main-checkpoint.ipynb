{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "In God We Trust\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CE417: Artificial Intelligence\n",
    "\n",
    "Dr. Mahdiyeh Soleymani Baghshah, Associate Professor\n",
    "\n",
    "Computer Engineering Department,\n",
    "Sharif University of Technology,\n",
    "Tehran, Tehran, Iran\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comment Classification (25 Points)\n",
    "\n",
    "Corresponding TA: Aryan Ahadinia\n",
    "\n",
    "In online retail stores, like digikala, people can leave comments on products and share their opinion. It's important to make sure that comment don't violet regulations so these website employ some people as comment reviewer to review comments one by and accept or reject comments. In this problem, we want to develop ML models to do comment reviewing task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You are denied to add any other packages.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data (1 Point)\n",
    "\n",
    "We want to work on data obtained from digikala. First, load train and test data separately. Then split train data into train data and validation data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(160000, 5)\n"
     ]
    }
   ],
   "source": [
    "#############################################\n",
    "#### Load Train and Test Data, CODE HERE ####\n",
    "#############################################\n",
    "train_data=pd.read_csv('data/train.csv')\n",
    "test_data=pd.read_csv('data/test.csv')\n",
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#### Split into train and validation, CODE HERE ####\n",
    "####################################################\n",
    "validation_size=0.3\n",
    "validation_indices=np.random.choice(len(train_data),int(len(train_data)*validation_size),replace=False)\n",
    "validation_data=train_data.iloc[validation_indices]\n",
    "train_data=train_data.drop(validation_indices,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                           title  \\\n",
      "0  12022                  پاور خوبی نیست   \n",
      "1  12023              حتما پیشنهاد میکنم   \n",
      "2  12025                اسپرسو ساز مباشی   \n",
      "4  12027                        عالی بود   \n",
      "6  12029  بهترین و کاربردی ترین پیتزا بر   \n",
      "\n",
      "                                             comment   rate  \\\n",
      "0                  خیلی دیر شارژ میشه. من پس فرستادم   15.0   \n",
      "1  اول قرار بود بخاری برقی یا فن هیتر بخریم ولی چ...   92.0   \n",
      "2  یک هفته پیش خریدم. یکی از دوستانم که کافی شاپ ...   96.0   \n",
      "4                       موزها تازه و قیمتم مناسب بود    0.0   \n",
      "6  برای پیتزا فقط این وسیله مناسب است که من دارم ...  100.0   \n",
      "\n",
      "   verification_status  \n",
      "0                    0  \n",
      "1                    0  \n",
      "2                    0  \n",
      "4                    0  \n",
      "6                    0  \n",
      "train data size: (112000, 5)\n",
      "validation data size: (48000, 5)\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "####### Show Train Data #######\n",
    "###############################\n",
    "print(train_data.head())\n",
    "print(f'train data size: {train_data.shape}')\n",
    "print(f'validation data size: {validation_data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                       title  \\\n",
      "0  18053  جنس تاریخ گذشته و فاسد شده   \n",
      "1   5088           کیفیت بسیار پایین   \n",
      "2   2264                    خوب نیست   \n",
      "3   3352                   بدنه ضعیف   \n",
      "4  10928         مناسب برای لباسشویی   \n",
      "\n",
      "                                             comment  rate  \n",
      "0  جنس تاریخ گذشته بود.حدود دو هفته می گذشت از ان...   0.0  \n",
      "1  من این محصول را امروز از دیجی کالا تحویل گرفتم...  25.0  \n",
      "2  اصلا خوب نیست، فقط در این حد خوبه که گوشی خامو...  14.0  \n",
      "3             کلا دو بار استفاده کردم بدنه مخزن شکست  64.0  \n",
      "4  فقط برای لباسشویی خوب است و در ظرفشویی بعد از ...   0.0  \n",
      "test data shape: (20000, 4)\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "####### Show Test Data ########\n",
    "###############################\n",
    "print(test_data.head())\n",
    "print(f'test data shape: {test_data.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning (5 Points)\n",
    "\n",
    "One of the most important steps im ML task is data cleaning. Data cleaning, generally, aim to transform data in a known domain in which is appropriate for the task. In this section, we explore some data cleaning techniques on text data. There are several libraries for text processing in persian like `hazm` and `parsi.io`. In this section, you can use these libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizations (1 Point)\n",
    "\n",
    "It is possible to have multiple form for a character or a word. For example some of characters like ک or ی in persian, has different encoding. In persian, the other problem is zero-width non joiner (ZWNJ) which may cause different written form of same words.\n",
    "\n",
    "Apply a text normalization on your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.1\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'inspect' has no attribute 'formatargspec'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m###################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#### Normalizations, CODE HERE ####\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m###################################\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhazm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Normalizer\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hazm\\__init__.py:2\u001b[0m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mWordTokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordTokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSentenceTokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTokenizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mTokenSplitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenSplitter\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\hazm\\WordTokenizer.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mcodecs\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m words_list, default_words, default_verbs\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TokenizerI\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mWordTokenizer\u001b[39;00m(TokenizerI):\n\u001b[0;32m     10\u001b[0m \t\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m\t>>> tokenizer = WordTokenizer()\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m\t>>> tokenizer.tokenize('این جمله (خیلی) پیچیده نیست!!!')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m\tدیگه میخوام ترک تحصیل کنم 😂 😂 😂\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m\t\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\__init__.py:115\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcollocations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m--> 115\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatstruct\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrammar\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:187\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(obj, name, default)\n\u001b[0;32m    185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m default\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;129;43m@decorator\u001b[39;49m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43mmemoize\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mgetattr_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemoize_dic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# memoize_dic is created at the first call\u001b[39;49;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:176\u001b[0m, in \u001b[0;36mdecorator\u001b[1;34m(caller)\u001b[0m\n\u001b[0;32m    174\u001b[0m     dec_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(src, \u001b[38;5;28mdict\u001b[39m(_func_\u001b[38;5;241m=\u001b[39mfunc, _call_\u001b[38;5;241m=\u001b[39mcaller))\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m update_wrapper(dec_func, func, infodict)\n\u001b[1;32m--> 176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdate_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_decorator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:87\u001b[0m, in \u001b[0;36mupdate_wrapper\u001b[1;34m(wrapper, model, infodict)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_wrapper\u001b[39m(wrapper, model, infodict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 87\u001b[0m     infodict \u001b[38;5;241m=\u001b[39m infodict \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mgetinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m=\u001b[39m infodict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     89\u001b[0m     wrapper\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m infodict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdoc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\decorators.py:69\u001b[0m, in \u001b[0;36mgetinfo\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m varkwargs:\n\u001b[0;32m     68\u001b[0m     argnames\u001b[38;5;241m.\u001b[39mappend(varkwargs)\n\u001b[1;32m---> 69\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[43minspect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformatargspec\u001b[49m(regargs, varargs, varkwargs, defaults,\n\u001b[0;32m     70\u001b[0m                                   formatvalue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m value: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# pypy compatibility\u001b[39;00m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(func, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'inspect' has no attribute 'formatargspec'"
     ]
    }
   ],
   "source": [
    "###################################\n",
    "#### Normalizations, CODE HERE ####\n",
    "###################################\n",
    "from hazm import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Normalizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming/Lemmatisation (2 Points)\n",
    "\n",
    "In many languages, like persian, arabic, and english, many words have a same root. Stemming and Lemmatisation are methods to transform all words is a family to a single form. In stemming we aim to find a common form for all word. It is not necessary to find the root as the common form. Moreover, it is possible to find the common form as a meaningless word. In lemmatisation we aim to find the root as the common form.\n",
    "\n",
    "Apply both Stemming and Lemmatisation on data. In following cells, you can choose to use which of the outputs at your own discretion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "####### Stemming, CODE HERE #######\n",
    "###################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "#### Lemmatisation, CODE HERE #####\n",
    "###################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words / Punctuations Removal (2 Points)\n",
    "\n",
    "In all languages, including Persian, there are very frequent words, including conjunctions, prepositions, and documentary verbs, which do not carry much meaning. Also, in normal natural language processing tasks, punctuation marks such as periods and commas are removed to clean the data.\n",
    "\n",
    "Remove stop words and punctuations in following cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "####### Stop Word Removal, CODE HERE #######\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "###### Punctuations Removal, CODE HERE #####\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes (5 Points)\n",
    "\n",
    "In this part, we want to implement a naive bayes classifier with assumption of independent distribution between features, which are tokens in our problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def fit(self, X, y):\n",
    "        ##################################\n",
    "        ######### YOUR CODE HERE #########\n",
    "        ##################################\n",
    "        pass\n",
    "\n",
    "    def predict(self, X):\n",
    "        ##################################\n",
    "        ######### YOUR CODE HERE #########\n",
    "        ##################################\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Vectorization (3 Points)\n",
    "\n",
    "There are several methods for generating a vectorized representations for tokens. Consider our token is words. and we want to develop a vectorizer to vectorize words. The main problem in vectorization is to maintain as much information as possible. A good assumptions is that frequency of a work represent the importance of that word. The assumption has a trivial exception. Stop words such as 'is', 'and', 'are', and e.t.c are very frequent. TF-TDF leverage term frequency and inverse document frequency to solve this problem. Document frequency shows that how frequents is a token among all documents. As the document frequency increases, the importance of that token drops. In the other hand, if a word has high frequency in a documents, it is likely to that word play a key role in that document so the importance of that token increases.\n",
    "\n",
    "In this section, you have to implement a TF-IDF vectorizer. Search about this method in Google and implement it.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_IDF:\n",
    "    def fit(self, X) -> None:\n",
    "        ##################################\n",
    "        ######### YOUR CODE HERE #########\n",
    "        ##################################\n",
    "        pass\n",
    "\n",
    "    def transform(self, X) -> np.ndarray:\n",
    "        ##################################\n",
    "        ######### YOUR CODE HERE #########\n",
    "        ##################################\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression (5 Points)\n",
    "\n",
    "In this part we want to train a logistic regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def _loss(self, X, y) -> float:\n",
    "        ##########################################################\n",
    "        ######### Calculate the loss function. CODE HERE #########\n",
    "        ######### You can use logarithm to avoid underflow #######\n",
    "        ##########################################################\n",
    "        pass\n",
    "\n",
    "    def _gradient(self, X, y) -> np.ndarray:\n",
    "        ##########################################################\n",
    "        ######### Calculate the gradient. CODE HERE ##############\n",
    "        ##########################################################\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y) -> None:\n",
    "        ##########################################################\n",
    "        ######### Train the model. CODE HERE #####################\n",
    "        ##########################################################\n",
    "        pass\n",
    "\n",
    "    def predict(self, X) -> np.ndarray:\n",
    "        ##########################################################\n",
    "        ######### Predict the result. CODE HERE ##################\n",
    "        ##########################################################\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation (3 Points)\n",
    "\n",
    "Now we want to evaluate our models on validation data. Calculate following metrics for both models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_real, y_predicted) -> float:\n",
    "    ##########################################################\n",
    "    ######### Calculate the accuracy. CODE HERE ##############\n",
    "    ##########################################################\n",
    "    pass\n",
    "\n",
    "\n",
    "def precision(y_real, y_predicted) -> float:\n",
    "    ##########################################################\n",
    "    ######### Calculate the precision. CODE HERE #############\n",
    "    ##########################################################\n",
    "    pass\n",
    "\n",
    "\n",
    "def recall(y_real, y_predicted) -> float:\n",
    "    ##########################################################\n",
    "    ######### Calculate the recall. CODE HERE ################\n",
    "    ##########################################################\n",
    "    pass\n",
    "\n",
    "\n",
    "def f1_score(y_real, y_predicted) -> float:\n",
    "    ##########################################################\n",
    "    ######### Calculate the f1_score. CODE HERE ##############\n",
    "    ##########################################################\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "######### Metrics for Naive Bayes ##########\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "######### Metrics for Logistic Regression ##\n",
    "############################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test data (3 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the final point, you have to run your model on test data and create a csv file with two columns: `id` and `verification_status`. Then run the `CommentJudge.jar` with following command and report your metrics. you must take an screenshot and import that in the notebook.\n",
    "\n",
    "```bash\n",
    "java -jar CommentJudge.jar ans.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################\n",
    "############ Code Here ############\n",
    "###################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
